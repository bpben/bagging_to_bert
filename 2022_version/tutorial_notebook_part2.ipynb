{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c2486-cf28-4e32-9b85-3caf4296a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb78f4-b61c-4e56-a1e2-153fb0482523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf78fc9-9ff7-4b8b-96ca-c852d808d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable \n",
    "# this will set the device on which to train\n",
    "#device = torch.device(\"cpu\")\n",
    "# if you're using GPU on Colab, this is how to set that device up\n",
    "# NOTE: GPU is recommended for this part\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219cfc03-000a-4381-a353-2778f97b2a36",
   "metadata": {},
   "source": [
    "## Bagging to BERT: A tour of applied NLP\n",
    "### Part 2: Beyond bagging\n",
    "### Table of Contents\n",
    "* [LSTM](#lstm)\n",
    "* [BERT](#bert)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7e591-7d7b-4d92-8f61-d5bb03af5dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data processing <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "Copied from part 1\n",
    "\n",
    "You'll either need to download the [imdb review data](https://ai.stanford.edu/~amaas/data/sentiment/) and save it to this directory OR download the [processed data](https://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharinghttps://drive.google.com/file/d/1oN_fO91IBkDHD_u6WXiUCvhhyNexQDJq/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c4ac0-5a90-49ef-b03b-378932d13c9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # processing the original data into DataFrame\n",
    "# # here for reference, don't need to run this if you're using reviews.pkl.gz\n",
    "# source_path = Path('./aclImdb/')\n",
    "# #neg_files = source_path.glob('./*/neg/*.txt')\n",
    "# #pos_files = source_path.glob('./*/pos/*.txt')\n",
    "# all_files = []\n",
    "# for f in source_path.glob('./*/*/*.txt'):\n",
    "#     filename = f.as_posix()\n",
    "#     if 'unsup' not in filename:\n",
    "#         # split up into useful components\n",
    "#         _, split, sent, idx = filename.split('/')\n",
    "#         idx = int(idx.split('_')[0])\n",
    "#         all_files.append([idx, split, sent, f.read_text()])\n",
    "# review_df = pd.DataFrame(all_files)\n",
    "# review_df.columns = ['idx', 'split', 'label', 'text']\n",
    "# # some minor html cruft is in here\n",
    "# review_df['text'] = review_df['text'].str.replace('<br /><br />', '')\n",
    "# review_df = review_df.to_pickle('reviews.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f61ea-8e62-4d21-8f5a-6e3eb1bed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can skip here if you already have reviews.pkl.gz\n",
    "review_df = pd.read_pickle('reviews.pkl.gz')\n",
    "review_df['label'] = review_df['label'] == 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f56964-2e4a-45bb-89d9-1da65ac6307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from part 1: using the same train/test split\n",
    "seed = 37\n",
    "np.random.seed(seed)\n",
    "pct_train = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_df['text'],\n",
    "    review_df['label'], train_size=pct_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746651a7-efe3-4098-a9c9-50d971baa113",
   "metadata": {},
   "source": [
    "### LSTM <a class=\"anchor\" id=\"lstm\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34655d0-8532-4ad3-9a46-02f9b2b390cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentLSTM(nn.Module):\n",
    "# adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(SentLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to sentiment space\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        # sigmoid activiation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass of nn\n",
    "        batch_size = x.shape[0]\n",
    "        # this is fit during training\n",
    "        embeds = self.word_embeddings(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # from lstm space to prediction space\n",
    "        pred_space = self.fc(lstm_out)\n",
    "        out = self.sigmoid(pred_space)\n",
    "        # reshape - want to get the prediction at end of seq\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out\n",
    "\n",
    "def doc_to_index(docs, vocab, tokenizer, unknown=1):\n",
    "    # transform docs into series of indices\n",
    "    docs_idxs = []\n",
    "    for d in docs:\n",
    "        w_idxs = []\n",
    "        d_tokenized = tokenizer(d)\n",
    "        for w in d_tokenized:\n",
    "            if w in vocab:\n",
    "                w_idxs.append(vocab[w])\n",
    "            else:\n",
    "                # unknown if not in vocab\n",
    "                w_idxs.append(unknown)\n",
    "        docs_idxs.append(w_idxs)\n",
    "    return(docs_idxs)\n",
    "\n",
    "def pad_sequence(seqs, seq_len=200):\n",
    "    # function for adding padding to ensure all seq same length\n",
    "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) != 0:\n",
    "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59664271-7328-4d64-a067-c9347adf70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to adapt vocab, leave space for padding\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.01)\n",
    "tfidf.fit(X_train)\n",
    "vocab = tfidf.vocabulary_\n",
    "tokenizer = tfidf.build_tokenizer()\n",
    "# need to add \"special\" tokens\n",
    "vocab = dict([(v, vocab[v]+2) for v in vocab])\n",
    "vocab['_UNK'] = 1\n",
    "vocab['_PAD'] = 0\n",
    "# from documents to vocab index\n",
    "parsed_train = doc_to_index(X_train, vocab, tokenizer)\n",
    "padded_train = pad_sequence(parsed_train)\n",
    "parsed_test = doc_to_index(X_test, vocab, tokenizer)\n",
    "padded_test = pad_sequence(parsed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df50502-d9f1-4704-8210-ebeeaa872bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct datasets for loading by PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(padded_train), \n",
    "                           torch.from_numpy(y_train.values))\n",
    "test_data = TensorDataset(torch.from_numpy(padded_test), \n",
    "                          torch.from_numpy(y_test.values))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,\n",
    "                         drop_last=True) # this is to keep the size consistent\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37691ceb-8429-43f6-a11a-9a4e55bfd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary problem, single output\n",
    "model_params = {'output_size': 1,\n",
    "               'hidden_dim': 512,\n",
    "               'embedding_dim': 400,\n",
    "               'vocab_size': len(vocab)}\n",
    "model = SentLSTM(**model_params)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbf0b2-a9ee-4511-a568-ea56767d4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr=0.005\n",
    "# binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# increasing this will make the training take a while on CPU\n",
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 5\n",
    "# gradient clipping: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # sets gradients to zero for each batch\n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "        # pred vs actual\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print progress\n",
    "        if counter%print_every == 0:\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in test_loader:\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out = model(inp)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fb9f2-0a0c-4530-a898-c8cc3d36f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch LSTM model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "model.eval()\n",
    "pred_collect = np.array([])\n",
    "eval_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "# eval mode\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in eval_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        # takes output, rounds to 0/1\n",
    "        pred = torch.round(output.squeeze())\n",
    "        pred_collect = np.concatenate([\n",
    "          pred_collect,\n",
    "          pred.cpu().numpy()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd2316-e1c8-4b49-b790-3a856c80ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy: {np.where(pred_collect == y_test)[0].shape[0]/y_test.shape[0]}')\n",
    "print(\n",
    "    classification_report(y_pred=pred_collect,\n",
    "                          y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9536ef-ae86-406f-92f2-8ef3fb07db18",
   "metadata": {},
   "source": [
    "### BERT <a class=\"anchor\" id=\"bert!pip install transformers\"></a>\n",
    "From [HF tutorials](https://huggingface.co/blog/sentiment-analysis-python).  The sentiment analysis pipeline packages together the tokenizer and the BERT model with a classification layer.  The default pipeline uses this [distilBERT model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48a465-92d4-44ab-9fb6-8eeefb3971ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will need to be run if you don't already have this package\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60eb292-ff25-4f5e-bf00-a7c8672c758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3e58f-5b03-41a2-92f7-69beb3fc0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some manipulations for speed and to play nice with BERT\n",
    "bert_pred = sentiment_pipeline(X_test.apply(lambda x: x).head(n=50).tolist())\n",
    "bert_pred = [p['label']=='POSITIVE' for p in bert_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afb1f9-1e29-463d-b4f9-50dcdd0c2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(bert_pred == y_test[:50])\n",
    "print(f'accuracy: {np.where(bert_pred == y_test[:50])[0].shape[0]/50}')\n",
    "print(\n",
    "    classification_report(y_pred=bert_pred,\n",
    "                          y_true=y_test[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba778396-c395-4364-a2dc-4aab7d274765",
   "metadata": {},
   "source": [
    "There it is - you've leveraged a cutting edge model to do sentiment analysis! This performance is pretty good, but our count vectors actually did a few points better.  Maybe there's an opportunity to fine-tune the BERT model specifically to the IMDB review dataset.  Let's try it.\n",
    "\n",
    "** IN PROGRESS **\n",
    "\n",
    "Still have been having some issues - performance is conspicuously low.  Use with caution.\n",
    "\n",
    "NOTE: This takes some time to run, even with the Collab GPU.  You might want to experiment on subsets of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d59127-dd97-4389-b000-0be0a48f7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# WIP - \"num_labels\" seems like it should be 1, have seen 2 (pos/neg)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",  num_labels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65798282-94d0-4e14-ac77-b19420828f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    return tokenizer(example, truncation=True)\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    # comes from https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.astype(float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# we will subsample because it makes this faster\n",
    "n = 1000\n",
    "tokenized_train = tokenizer(X_train.tolist()[:n], \n",
    "                            truncation=True, padding=True)\n",
    "tokenized_test = tokenizer(X_test.tolist()[:n], \n",
    "                           truncation=True, padding=True)\n",
    "\n",
    "train_dataset = IMDbDataset(tokenized_train, y_train.values[:n])\n",
    "test_dataset = IMDbDataset(tokenized_test, y_test.values[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c0157-ed4b-434d-bafa-215c8a87f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using HF's trainer\n",
    "# again, this comes from https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir='./',\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset,\n",
    "   eval_dataset=test_dataset,\n",
    "   tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec1c8f-f444-4b02-88bb-0fb6b94d23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1068ef-f95d-464e-af73-184825f79c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacky - moving back to cpu where the other data is located\n",
    "# HF suggests a \"prediction\" Trainer\n",
    "m = model.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197342ef-d267-4ba4-9a27-73e25f369e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same parameters we trained with\n",
    "classifier_pipeline = pipeline(task=\"text-classification\", \n",
    "                               model=m, tokenizer=tokenizer,\n",
    "                               truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e09bf6-cf5f-4490-afcd-ec322b973041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred outputs slightly different from sentiment pipeline\n",
    "bert_pred = classifier_pipeline(X_test.tolist())\n",
    "bert_pred = [p['score']>0.5 for p in bert_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b4fcd-3b7b-43f5-85a9-45d7d424dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance is pretty bad, even after 5 epochs\n",
    "print(f'accuracy: {np.where(bert_pred == y_test)[0].shape[0]/len(y_test)}')\n",
    "print(\n",
    "    classification_report(y_pred=bert_pred,\n",
    "                          y_true=y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy3",
   "language": "python",
   "name": "spacy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
